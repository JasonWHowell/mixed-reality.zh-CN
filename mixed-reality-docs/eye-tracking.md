---
title: 眼睛-注视
description: HoloLens 2 为开发人员提供了使用用户所注视对象的相关信息的功能，将全息体验中的环境和人类理解能力提高到了一个新境界。
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: 眼睛跟踪，混合现实，输入，眼睛
ms.openlocfilehash: 51779b7b210522aa4d19b5a32d7df6ccb2cb3a35
ms.sourcegitcommit: ff330a7e36e5ff7ae0e9a08c0e99eb7f3f81361f
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/28/2019
ms.locfileid: "70122062"
---
# <a name="eye-gaze-on-hololens-2"></a>目视-注视 HoloLens 2
HoloLens 2 为开发人员提供了使用用户所注视对象的相关信息的功能，将全息体验中的环境和人类理解能力提高到了一个新境界。 本页告诉开发人员如何从各种用例的目视跟踪中获益，以及在设计基于目视的用户界面时要查找的内容。 


## <a name="device-support"></a>设备支持

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><strong>功能</strong></td>
     <td><a href="hololens-hardware-details.md"><strong>HoloLens（第 1 代）</strong></a></td>
     <td><strong>HoloLens 2</strong></td>
     <td><a href="immersive-headset-hardware-details.md"><strong>沉浸式头戴显示设备</strong></a></td>
</tr>
<tr>
     <td>眼睛-注视</td>
     <td>❌</td>
     <td>✔️</td>
     <td>❌</td>
</tr>
</table>

## <a name="use-cases"></a>用例
眼动跟踪可让应用程序实时跟踪用户正在注视的位置。 以下用例介绍了混合现实中的目视跟踪可能会发生的一些交互。
请记住，[混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)对于使用目视跟踪提供几个有趣和强大的示例非常有用，例如，快速和轻松地支持目视的目标选项，以及基于用户查看的内容。 

### <a name="user-intent"></a>用户意图    
有关用户所在位置和内容的信息**为其他输入**提供了强大的上下文，例如语音、动手和控制器。
可在各种任务中使用此信息。
例如，这种方法的范围包括：只需查看一个全息影像并口述 "select" （也可以说 "选择["），](gaze-and-commit.md)或者说 *"put ..."* ，然后查看用户想要放置全息图，并说 *"。出现 "* 。 在[混合现实工具包 - 视线支持的目标选择](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)和[混合现实工具包 - 视线支持的目标定位](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)中可以找到相关示例。

此外，用户意向的示例可能包括使用用户查看的信息来增强使用所介绍的虚拟代理和交互式全息影像。 例如，虚拟代理可能会根据当前查看的内容来调整可用选项及其行为。 

### <a name="implicit-actions"></a>隐式操作
隐式操作的类别与用户意图密切相关。
其思路是，全息影像或用户界面元素以某种 instinctual 的方式做出反应，甚至可能不会像用户同时与系统交互，而是让系统和用户保持同步。例如，**基于目视的自动滚动**，用户可以在其中读取长文本，该文本会在用户进入文本框底部时自动开始滚动，以使用户处于阅读流中而不抬起手指。  
这种情况的一个关键方面是，滚动速度可适应用户的读取速度。
另一个例子就是**受支持的目视缩放和平移**，用户可以感觉到他或她所关注的内容完全接近。 触发缩放和控制缩放速度可以通过语音或手写输入来控制，这对于向用户提供控制感受，同时避免被淹没非常重要。 下面将更详细地讨论这些设计指南。 放大后，用户可以顺利地执行操作，例如，街道的学习过程只需使用其眼睛来浏览其邻居即可。
[混合现实工具包 - 视线支持的导航](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)示例中可以找到此类交互的演示。

_隐式操作_的其他用例包括：
- **智能通知：** 当你专心浏览某段内容时，突然弹出的通知是否让你觉得很恼火？ 考虑到用户正在关注的内容，你可以通过从当前 gazing 用户的位置偏移通知来更好地进行此体验。 这会限制干扰，并在用户完成读取后自动将其关闭。 
- **体贴入微的全息影像：** 在 gazed 时，会对影像进行细微反应。 这种情况的范围包括：从稍微光亮的 UI 元素到缓慢的百花齐放花给虚拟宠物，开始回顾用户，或尝试避免在长时间的琢磨后出现用户的眼睛。 这种交互可能会在应用程序中提供更有趣的连接和满意度。

### <a name="attention-tracking"></a>注意力跟踪   
有关用户所在位置或内容的信息是一个非常强大的工具，可用于评估设计的可用性，并确定有效工作流中的问题。 目视跟踪可视化和分析是各种应用程序领域的常见做法。 在 HoloLens 2 中，我们提供了一种新的维度来理解，因为3D 全息图可以放置在真实的上下文中并进行相应的评估。 [混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)提供了用于记录和加载目视跟踪数据的基本示例，以及如何对其进行可视化。

此区域中的其他应用程序可以包括： 
-   **远程目视视觉视觉对象：** 可视化远程协作者正在寻找的内容，以确保正确理解并遵循说明。
-   **用户调研：** 注意跟踪可用于探索初级用户与专家用户直观地分析内容的方式，或者对复杂任务（例如分析医疗数据或操作机械）进行实时协调。
-   **训练模拟和性能监视：** 更有效地识别执行流中的瓶颈，实践并优化任务的执行。
-   **设计评估、广告和营销调查：** 在评估网站和产品设计时，目视跟踪是一种用于市场研究的常用工具。

### <a name="additional-use-cases"></a>其他用例
- **游戏：** 是否曾经想过拥有超能力？ 机会来了！ 可以通过起始在 levitate 全息影像。 从眼睛发射激光束。 将敌人变成石子，或将其冻结。 使用 X 光透视来扫描建筑物。 没有做不到，只有想不到！  

- **富有表现力的虚拟形象：** 目视跟踪通过使用实时眼睛跟踪数据来使真实的头像显示用户正在查看的内容，以更有意义的3D 头像为目标。 

- **文本输入：** 目视跟踪可用作低工作量文本输入的替代方案，特别是当语音或手不方便使用时。 


## <a name="available-eye-tracking-data"></a>可用的目视跟踪数据
在深入了解有关目视目视交互的特定设计准则之前，我们想要简要指出 HoloLens 2[目视跟踪 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)提供的功能。 开发人员可在大约_30 FPS （60 Hz）_ 上访问一只眼睛为眼睛的光线（注视原点和方向）。
有关如何访问目视跟踪数据的更多详细信息，请参阅我们的开发人员指南了解如何使用[DirectX 中的眼睛](gaze-in-directx.md)，并看看[Unity](https://aka.ms/mrtk-eyes)。

围绕实际目标围绕视觉角度，眼睛的眼睛约在1.5 度内（请参阅下图）。 由于预期的轻微 imprecisions，开发人员应计划围绕此下限值（例如 2.0-3.0 度）的某些边距，这可能会导致更舒适的体验。 下面将详细介绍如何处理小目标的选择。 要准确运行眼动跟踪，每个用户需要完成眼动跟踪用户校准。 

![2 米远处的最佳目标大小](images/gazetargeting-size-1000px.jpg)<br>
*以2米距离为目标的最佳目标大小*

## <a name="calibration"></a>校准 
为了使目视跟踪能准确地工作，每个用户都需要经历[跟踪用户校准](calibration.md)，用户必须查看一组全息目标。 这允许设备调整系统，以便为用户提供更舒适和更高的质量查看体验，并确保同时进行准确的目视跟踪。 目视跟踪应该适用于大多数用户，但在某些情况下，用户可能无法成功校准。
若要了解有关校准的详细信息，请查看[校准](calibration.md)。

## <a name="eye-gaze-input-design-guidelines"></a>目视输入设计准则
构建充分利用快速移动目视目标的交互可能会很困难。 在本部分中，我们总结了在设计应用程序时要考虑的主要优点和挑战。 

### <a name="benefits-of-eye-gaze-input"></a>目视输入的优点
- **高速指向。** 眼睛判断力是人为身体最快的响应判断力。 

- **不费力。** 几乎没有任何身体动作。 

- **隐含性。** 用户通常会将其描述为 "构思阅读"，有关用户的目视变动的信息使系统可以知道用户打算参与哪个目标。 

- **备选的输入通道。** 眼睛可为用户提供强大的支持输入，并根据用户的手眼协调从用户的经验中生成。

- **视觉注意力。** 另一个重要的优点是可以推断出用户正在关注的内容。 这可以帮助不同的应用程序领域，从更有效地评估不同设计，到协助的用户界面，增加了社交提示以进行远程通信。

简而言之，使用眼睛眼睛作为输入可提供快速且简单的上下文信号。 与其他输入（如*语音*和*手动*输入）相结合时，此功能特别强大，可以确认用户的意图。


### <a name="challenges-of-eye-gaze-as-an-input"></a>眼睛为输入的挑战
如果有大量的强大功能，就有了许多责任。
虽然眼睛可用于创建符合 superhero 的用户体验，但若要清楚地说明这一点，还必须了解这一点。 下面讨论了一些需要考虑的问题，以及如何解决这些*问题*： 

- **眼睛为 "始终打开"** 打开眼睛护盖后，眼睛开始 fixating 环境中的东西。 对每个外观进行操作并意外发出操作，因为您查看的内容太长会导致 unsatisfying 体验。
因此，建议将眼睛与*声音命令*、*笔势*、*按钮单击*或扩展停留结合起来，以触发选择目标。
此解决方案还允许使用一种模式，在该模式下，用户可以自由地查找，而不会因触发某些事情而 involuntarily。 如果仅查看目标，则在设计视觉对象和听觉反馈时，还应考虑此问题。
不要使用即时弹出式效果或惊悚的声音来让用户感到不知所措。 个很微妙为 key。 稍后在谈到设计建议时，我们会进一步讨论一些最佳做法。

- **观察与控制**假设您想要在墙壁上精确地伸直照片。 你会参照它的边框和四周，检查它是否平齐。 现在，如果您想要使用眼睛作为输入来移动图片，则可以想象出如何实现此目的。 有点难度，对不对？ 这介绍了输入和控制需要时，眼睛的双重角色。 

- **点击之前离开目标：** 对于快速目标选择，研究表明，用户的眼睛可以在结束手动单击之前继续进行（例如，airtap）。 因此，必须特别注意的是，将快速目视眼睛的信号与慢速控制输入（例如语音、双手、控制器）进行同步。

- **小目标：** 当您尝试读取只是太小而无法阅读的文本时，您是否知道这种感觉？ 这会使您感到非常紧张，因为您尝试重新调整您的眼睛，使您能够更好地专注。
当你强制用户使用目视目标选择在你的应用程序中太小的目标时，你可以在用户中调用这种感觉。
在设计方面，为了给用户建立一种愉悦舒适的体验，我们建议目标至少在 2° 的视角范围内，最好是再大一些。

- **眼睛不规则的运动**我们的眼睛执行从固定到固定的快速移动。 观察录制的眼部运动扫描路径时你会发现，这些路径看起来是不规则的。 您的眼睛会在与*打印头*或*手间运动*比较的情况中快速跳转。  

- **跟踪可靠性：** 当眼睛在光线变化的环境中适应新的条件时，眼动跟踪准确度可能会略有下降。
虽然这应该不会影响应用程序的设计，因为准确性应该在2°限制范围内，因此，用户可能需要重新校准。 


## <a name="design-recommendations"></a>设计建议
下面是基于目视输入的所述优点和挑战的特定设计建议列表：

1. **眼睛看不到打印头：**
    - **考虑快速且不规则的眼部运动是否适合你的输入任务：** 虽然我们的快速而引人注目的变动非常适合于在我们的视图中快速选择目标，但它不适用于需要平滑输入轨迹（如绘图或 encircling 批注）的任务。 在这种情况下，应该首选手部或头部指向。
  
    - **避免直接将某些内容附加到用户的眼睛，如滑块或光标。**
如果是游标，这可能会导致 "fleeing cursor" 效果，原因是预计的眼睛眼睛略有偏差。 如果是滑块，则它可能与用眼睛控制滑块的双角色冲突，同时也需要检查对象是否位于正确的位置。 简而言之，用户可能会被淹没和分散注意力，尤其是在信号对于该用户不精确的情况下。 
  
2. **将眼睛与其他输入组合在一起：** 将眼睛跟踪与其他输入（如手势、语音命令或按钮按下）的集成具有以下优点：
    - **可以自由观察：** 考虑到我们的主要角色是观察我们的环境，这是一项重要的用户，而不触发任何（视觉、听觉等）反馈或操作。 
    将目视跟踪与其他输入控件结合起来允许在目视跟踪观察和输入控制模式之间平稳过渡。
  
    - **强大的上下文提供程序：** 使用有关用户在 uttering 语音命令或执行手手势时所处的位置和内容的信息，可以在视图字段间无缝排列输入。 例如：发出“放在这里”语音命令后，只需注视某个目标和目的地，就能快速顺畅地在场景中选择和放置全息影像。 

    - **同步多模输入的需求（“点击之前离开”问题）：** 将快速目视运动与更复杂的附加输入（例如长的语音命令或手势）相结合，就能在完成附加输入命令之前继续眼睛的风险。 因此，如果您创建自己的输入控件（例如，自定义笔势），请确保记录此输入或大致持续时间的开始，以将其与用户过去查看的内容相关联。
    
3. **眼动追踪输入的微妙反馈：** 当查看目标以指示系统正在按预期工作但应保持微妙时，提供反馈很有用。 这可能包括缓慢混合、放大和缩小、视觉对象突出显示或执行其他微妙目标行为（如缓慢增加目标大小），以指示系统正确检测到用户正在查看目标而不不必要地中断用户的当前工作流。 

4. **避免强制使用非自然的眼部运动作为输入：** 不要强制用户执行特定的目视运动（注视手势）来触发应用程序中的操作。

5. **考虑不精确性：** 我们区分对用户显而易见的两种类型的 imprecisions：偏移和抖动。 解决偏移量的最简单方法是提供足够大的目标来与进行交互。 建议使用大于2°的视觉角度作为参考。 例如，当您拉伸 arm 时，缩略图约为2°。 因此，我们的指导如下：
    - 不要强制用户选择 "小目标"。 研究表明，如果目标足够大，并且系统设计良好，则用户会将其交互描述为轻松和神奇。 如果目标太小，则用户就会将体验描述为费力、令人沮丧。
  
## <a name="dev-guidance-what-if-eye-tracking-is-not-available"></a>开发指南：如果眼睛跟踪不可用，该怎么办？
在某些情况下，由于各种原因（包括但不限于），应用无法接收任何目视跟踪数据：
* 用户跳过了眼睛跟踪校准。
* 用户进行了校准，但决定不向应用程序授予使用其眼跟踪数据的权限。
* 用户具有唯一的眼镜，或系统尚不支持的某种眼睛状态。
* 外部因素抑制了一种可靠的眼睛跟踪，如在眼睛前面的头发上出现污迹面板或眼镜、强烈直接阳光和 occlusions。

作为应用开发人员，这意味着你需要考虑如何支持目视跟踪数据可能不可用的用户。 下面我们首先介绍如何检测目视跟踪是否可用，以及如何解决不能用于不同应用程序的情况。

### <a name="1-how-to-detect-that-eye-tracking-is-available"></a>1.如何检测目视跟踪可用
可以通过几个检查来确定是否有目视跟踪数据可用。 检查是否 。
* ...系统完全支持目视跟踪。 调用以下*方法*：[EyesPose. IsSupported （）](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.issupported#Windows_Perception_People_EyesPose_IsSupported)

* ...校准了用户。 调用以下*属性*：[EyesPose. IsCalibrationValid。](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose.iscalibrationvalid#Windows_Perception_People_EyesPose_IsCalibrationValid)

* ...用户已将你的应用权限授予使用其目视跟踪数据：检索当前的 _"GazeInputAccessStatus"_ 。 有关如何执行此操作的示例，请参阅[请求访问注视输入](https://docs.microsoft.com/en-us/windows/mixed-reality/gaze-in-directX#requesting-access-to-gaze-input)。

此外，你可能需要通过在收到的目视跟踪数据更新之间添加超时，并以其他方式回退到下面所述的打印头来检查眼睛跟踪数据是否过时。 

如上所述，眼睛跟踪数据可能不可用的原因有多种。 尽管某些用户可能已特意决定撤消对其眼睛跟踪数据的访问，但对于不提供对其眼睛跟踪数据的访问权限的隐私性的用户体验的不满，这一点很有可能是不可能的。 因此，如果你的应用程序使用目视跟踪，并且这是体验的重要部分，我们建议你清楚地将此信息传递给用户。 请通知用户，目视跟踪对于你的应用程序至关重要（甚至可能会列出一些增强的功能）以充分利用你的应用程序，从而帮助用户更好地了解他们所放弃的内容。 帮助用户确定目视跟踪可能不工作的原因（基于上述检查），并提供一些建议来快速解决潜在问题。 例如，如果您可以检测到系统支持目视跟踪，则用户会进行校准，甚至会获得其权限，但不会收到任何眼睛跟踪数据，这可能会导致某些其他问题，如污迹或眼睛封闭像素。 但请注意，在某些情况下，眼睛跟踪可能只是不能正常工作。 因此，在应用中启用眼睛跟踪时，可以通过允许消除甚至禁用提醒来过于这种情况。

### <a name="2-fallback-for-apps-using-eye-gaze-as-a-primary-input-pointer"></a>2.使用红眼作为主要输入指针的应用回退
如果你的应用程序使用目视看眼作为指针输入来快速选择场景中的全息影像，但眼睛跟踪数据不可用，则建议回退到打印头并开始显示眼睛良好的光标。 建议使用超时值（例如500–1500毫秒）来确定是否要切换。 这是为了防止在系统每次由于快速目视动作或传情动漫或传情动漫而发生跟踪时弹出游标。 如果你是一名 Unity 开发人员，则已在混合现实工具包中处理自动回退到打印头。 如果你是 DirectX 开发人员，则需要自行处理此开关。

### <a name="3-fallback-for-other-eye-tracking-specific-applications"></a>3.其他目视跟踪特定应用程序的回退
您的应用程序可以采用专门为眼睛定制的独特方式（例如，为头像形象提供动画或基于眼睛的关注热图，它依赖于有关视觉对象的精确信息）。 在这种情况下，不会有任何明确的回退。 如果目视跟踪不可用，则可能只需禁用这些功能。 

<br>

在此页面中，你可以获得一个良好的概述，使你开始了解 HoloLens 2 的眼睛跟踪和目视眼睛输入的角色。 若要开始开发，请查看我们关于[Unity 中眼睛](https://aka.ms/mrtk-eyes)的信息并观看[DirectX 中的眼睛](gaze-in-directx.md)。


## <a name="see-also"></a>请参阅
* [目视观察 DirectX](gaze-in-directx.md)
* [目视看 Unity （混合现实工具包）](https://aka.ms/mrtk-eyes)
* [校准](calibration.md)
* [头部凝视并提交](gaze-and-commit.md)
* [手势](gestures.md)
* [语音输入](voice-design.md)
* [运动控制器](motion-controllers.md)
* [舒适](comfort.md)
