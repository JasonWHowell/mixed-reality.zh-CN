---
title: 眼动跟踪
description: 眼动跟踪
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: 眼动跟踪, 混合现实, 输入, 视线
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 06/05/2019
ms.locfileid: "66453701"
---
# <a name="eye-tracking-on-hololens-2"></a>HoloLens 2 中的眼动跟踪
HoloLens 2 提供令人难以置信的功能来让开发人员利用用户正在注视的对象的相关信息，它将全息体验中的上下文和人类理解能力提高了一个全新的境界。 本网页概述开发人员如何在不同的用例中利用眼动跟踪，以及在设计基于视线的用户界面时要注意哪些问题。 

## <a name="use-cases"></a>用例
眼动跟踪可让应用程序实时跟踪用户正在注视的位置。 本部分介绍混合现实中的眼动跟踪可以实现的一些潜在用例以及新颖的交互方式。
在开始之前，请注意本文稍后会多次提到[混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)，其中提供了多个有趣且知识点非常丰富的示例，用于演示眼动跟踪的用法，例如，快速轻松地选择视线支持的目标，以及根据用户注视的位置自动滚动文本。 

### <a name="user-intent"></a>用户意图    
有关用户注视位置的信息为语音、手和控制器等**其他输入**提供了丰富的上下文。
可在各种任务中使用此信息。
例如，这些任务可能包括，注视某个全息影像并讲出“选择”（另请参阅[头部跟踪视线和提交](gaze-and-commit.md)），或者讲出“将这个...”，然后注视要将该全息影像放到的某个位置并讲出“...放到这里”，即可快速轻松地在场景中切换**目标位置**。 在[混合现实工具包 - 视线支持的目标选择](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)和[混合现实工具包 - 视线支持的目标定位](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)中可以找到相关示例。

用户意图的其他示例可能包括，使用用户注视的对象的相关信息的信息来增强具体化的虚拟代理和交互式全息影像的互动能力。 例如，虚拟代理能够根据当前查看的内容调整可用选项及其行为。 

### <a name="implicit-actions"></a>隐式操作
隐式操作的类别与用户意图密切相关。
其思路是，全息或用户界面元素凭借某种本能做出反应，就好像用户根本不是在与系统交互，而像是两者已同步。例如，一个非常成功的例子是**基于视线的自动滚屏**。 其思路非常简单：用户可以不中断地阅读文本。 文本会根据用户的阅读进度逐渐移动。 一个重要的特征是，滚屏速度与用户的阅读速度相适应。
另一个例子是**视线支持的缩放和平移**：用户可以感受到他们所要关注的内容立即涌现在眼前。 可以通过语音或手动输入来触发缩放和控制缩放速度，这对于提供控制感和避免出现眼花缭乱方面非常重要（下面会更详细地讨论这些设计准则）。 例如，在放大后，用户可以通过视线顺畅地观看其周边的街景。
[混合现实工具包 - 视线支持的导航](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)示例中可以找到此类交互的演示。

隐式操作的其他用例包括： 
- **智能通知：** 当你专心浏览某段内容时，突然弹出的通知是否让你觉得很恼火？ 根据用户当前正在专心浏览的位置，你可以让事情变得更美好！ 可以在偏离用户当前正在观看的位置显示通知，以避免分散其注意力，并在他们阅读完通知后自动隐藏通知。 
- **体贴入微的全息影像：** 注视时可做出微秒反应的全息影像。 这种反应包括柔和地让 UI 元素发光、当虚拟宠物开始回头看你时缓慢绽放花朵，或者在你长时间凝视后尝试避开你的视线。 这可以在应用中提供一种有趣的互动感和满足感。

### <a name="attention-tracking"></a>注意力跟踪   
有关用户注视位置的信息是评估设计可用性以及在有效工作流中识别问题的强有力手段。 到目前为止，眼动追踪可视化和分析在各种应用领域中司空见惯。 HoloLens 2 为这种认知提供了一个新的维度，3D 全息影像可以放在真实的上下文中，并可以一起评估。 [混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)提供了有关记录、加载和可视化眼动跟踪数据的基本示例。

此领域的其他应用场景包括： 
-   **远程视线可视化：** 例如，可视化远程协作者正在注视的内容，以确保正确理解并遵循指示。
-   **用户调研：** 注意力跟踪可用于探索新手与专家用户对内容进行视觉分析的方式，或者在复杂任务中的手眼协调性（例如，分析医疗数据或操作机器时）。
-   **训练模拟和性能监视：** 更有效地识别执行流中的瓶颈，实践并优化任务的执行。
-   **设计评估、广告和营销调查：** 眼动跟踪是市场调查中评估网站和产品设计的一种常用工具。

### <a name="additional-use-cases"></a>其他用例
- **游戏：** 是否曾经想过拥有超能力？ 机会来了！ 注视全息影像，使之漂浮起来。 从眼睛发射激光束。 将敌人变成石头，或将其冻住！ 使用 X 光透视来扫描建筑物。 没有做不到，只有想不到！  

- **富有表现力的虚拟形象：** 眼动跟踪有助于创建更富表现力的 3D 虚拟形象，因为它可以使用实时眼动跟踪数据来制作虚拟形象的眼睛动画，指示用户当前正在注视哪些内容。 它还能添加更多的眨单眼和眨眼表情。 

- **文本输入：** 可将眼动跟踪用作一种较为轻松的文本输入方式，而且输入过程比较有趣，尤其是不方便使用语音或双手时。 


## <a name="eye-tracking-api"></a>眼动跟踪 API
在详细了解视线交互的具体设计准则之前，我们希望简要描述一下 HoloLens 2 眼动跟踪器所提供的功能。 可通过 `Windows.Perception.People.EyesPose` 访问[眼动跟踪 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose)。 它向开发人员提供单一视线（视线原点和方向）。
眼动追踪器以大约 _30 FPS_ 的速率提供数据。
预测的视线位于 围绕实际注视目标的圆周 1.0 - 1.5 度视角范围内。 预期存在轻微的不精确性，因此，应该围绕此下限值规划好一定的余隙。 下面会更详细地讨论此问题。 要准确运行眼动跟踪，每个用户需要完成眼动跟踪用户校准。 

![2 米远处的最佳目标大小](images/gazetargeting-size-1000px.jpg)<br>
*2 米远处的最佳目标大小*


## <a name="eye-gaze-design-guidelines"></a>视线设计准则
构建利用快速移动的视线定位功能的交互式应用可能有难度。 本部分汇总了视线输入的主要优势，以及在设计应用时需要考虑到的一些难题。 

### <a name="benefits-of-eye-gaze-input"></a>视线输入的优势
- **高速指向。** 眼部肌肉在人体中是反应速度最快的肌肉。 

- **不费力。** 几乎没有任何身体动作。 

- **隐含性。** 此功能通常被用户描述为“读心术”。有关用户眼部动作的信息可让系统知道用户想要与哪个目标互动。 

- **备选的输入通道。** 视线跟踪可为多年以来根据用户手眼协调性实现的手写和语音输入体验提供强有力的支持。

- **视觉注意力。** 另一个重要优势是能够推断出用户正在关注哪种内容。 这可以在各种应用领域中提供帮助，包括有效评估不同的设计，以帮助开发出更智能的用户界面，以及增强社交手段来实现远程通信。

简单地说，使用视线作为输入有可能会提供快速且毫不费力的上下文信号 - 与其他输入（例如，使用语音和人工输入来确认用户意图）结合使用时，此功能尤为强大。  


### <a name="challenges-of-eye-gaze-as-an-input"></a>将视线用作输入所带来的难题
能力越大，责任也越大：尽管视线跟踪像超级英雄一样可以打造梦幻的用户体验，但相应地考虑到它的不足之处也很重要。 下面讨论在使用视线输入时要考虑到的一些难题，以及如何解决这些难题：  

- **视线始终是开启的。** 张开双眼的那一刻，眼睛就会开始注视环境中的事物。 对每种表情作出的反应，以及由于长时间注视某个事物而意外产生的动作都会导致出现可怕的体验！
正因如此，我们建议将视线跟踪与语音命令、手势、按钮点击或延长的停顿相结合来触发目标选择。   
这种解决方法还可以让用户进入一种可以自由环顾的模式，而不会迫使他们不情愿地触发某个操作。 设计视觉和听觉反馈（只需注视某个目标）时，也应考虑到此问题。
不要使用即时弹出式效果或惊悚的声音来让用户感到不知所措。 微妙才是关键所在！ 稍后在谈到设计建议时，我们会进一步讨论一些最佳做法。

- **观测与控制。** 假设你要在墙上精确地将某幅照片摆放整齐。 你会参照它的边框和四周，检查它是否平齐。 现在想一想，如果你同时要使用视线作为输入来移动图片，如何做到平齐地移动呢？ 有点难度，对不对？ 这就是视线在同时用于输入和控制时发挥的双重作用。 

- **点击之前离开目标：** 在快速目标选择方面，有研究表明，在确定手动点击（例如隔空敲击）之前，用户的视线可能会移动。 因此，将快速视线信号与慢速控制输入（例如语音、手、控制器）同步时，必须特别小心。

- **小目标：** 你是否体会过阅读的文字太小，以致不能舒适阅读时的那种感受？ 眼睛的紧张感会让你感到疲倦，因为你要尝试重新调整双眼，以便更好地聚焦。
如果在应用中使用视线定位来强迫用户选择太小的目标，则用户可能就会产生这种感觉。
在设计方面，为了给用户建立一种愉悦舒适的体验，我们建议目标至少在 2° 的视角范围内，最好是再大一些。

- **不规则的视线移动。** 我们的眼睛会从一个位置快速移动到另一个位置。 观察录制的眼部运动扫描路径时你会发现，这些路径看起来是不规则的。 与头部跟踪视线和手部运动相比，眼睛的移动速度飞快，是本能跳动的。    

- **跟踪可靠性：** 当眼睛在光线变化的环境中适应新的条件时，眼动跟踪准确度可能会略有下降。
这不一定会影响应用的设计，因为准确度应该处于上述 2° 限制范围内。 但这可能意味着，用户需要再次运行校准。 


### <a name="design-recommendations"></a>设计建议
下面我们根据所述的视线输入优势和难题列出了具体的设计建议：

1. **视线 != 头部跟踪视线：**
    - **考虑快速且不规则的眼部运动是否适合你的输入任务：** 尽管快速且不规则的眼部运动非常适合用于快速选择视场中的目标，但它不是很适合用于需要平滑输入轨迹的任务（例如，绘图或圈住批注）。 在这种情况下，应该首选手部或头部指向。
  
    - **避免将物件直接对准用户的视线（例如滑块或光标）。**
以光标为例，这可能会导致“光标逃逸”效应，因为投影的视线信号中存在微小的偏移。 以滑块为例，它会与视线既要通过眼睛控制滑块，又要检查对象是否位于正确的位置这种双重角色相冲突。 简单而言，用户可能很快就会感到眼花缭乱，注意力分散，尤其是信号对于该用户不精确时。 
  
2. **将视线与其他输入相结合：** 将眼动跟踪与其他输入（例如手势、语音命令或按钮）相集成可带来以下优势：
    - **可以自由观察：** 假设我们的眼睛主要用于观察环境，则必须允许用户在不触发任何（视觉、听觉...）反馈或操作的情况下环顾四周。 
    将 ET 与其他输入控制相结合，可在 ET 观察与输入控制模式之间平稳过渡。
  
    - **强大的上下文提供程序：** 在发出语音命令或执行手势的同时使用有关用户正在注视哪个位置的信息，可以毫不费力地在整个视场中传送输入。 示例包括：发出“放在这里”语音命令后，只需注视某个目标和目的地，就能快速顺畅地在场景中选择和放置全息影像。 

    - **同步多模输入的需求（“点击之前离开”问题）：** 将快速眼部运动与更复杂的附加输入（例如，较长的语音命令或手势）相结合会带来在完成附加输入命令之前移动视线的风险。 因此，如果你要创建自己的输入控件（例如自定义手势），请务必记录此输入的发生时间或近似持续时间，使之与用户过去注视的物件相关联。
    
3. **眼动追踪输入的微妙反馈：** 如果某个目标已被注视（表示系统正在按预期方式工作）但应保持微妙的状态，则最好是提供反馈。 这可能包括，慢慢融入/排除视觉关注点，或执行其他微妙的目标行为，例如慢动作（例如略微增大目标），以指示系统已正确检测到用户正在注视目标，但是，不要无谓地中断用户的当前工作流。 

4. **避免强制使用非自然的眼部运动作为输入：** 不要强迫用户执行特定的眼部运动（注视手势）来触发应用中的操作。

5. **考虑不精确性：** 我们区分两种对用户而言非常重要的不精确性：偏移和抖动。 解决偏移的最简单方法是提供足够大的交互目标（视角 2° – 供参考：张开双臂 (1) 时，你的缩略图的视角大约为 2°）。 因此，我们的指导如下：
    - 不要强迫用户选择微小的目标：研究表明，如果目标足够大（并且系统设计得当），则用户就会将交互效果描述为轻松且神奇。 如果目标太小，则用户就会将体验描述为费力、令人沮丧。
   

## <a name="see-also"></a>另请参阅
* [头部凝视并提交](gaze-and-commit.md)
* [DirectX 中的头部和眼部凝视](gaze-in-directx.md)
* [Unity（混合现实工具包）中的视线跟踪](https://aka.ms/mrtk-eyes)
* [手势](gestures.md)
* [语音输入](voice-design.md)
* [运动控制器](motion-controllers.md)
* [舒适](comfort.md)
