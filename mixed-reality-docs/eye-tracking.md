---
title: 眼睛-注视
description: HoloLens 2 允许开发人员使用有关用户正在查看的信息的功能, 使开发人员能够在全息体验内实现新的上下文和人工理解。
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
keywords: 眼睛跟踪, 混合现实, 输入, 眼睛, 眼睛, 眼睛
ms.openlocfilehash: c847f7de2cf4492c89225a88aeaf189f51cfbc40
ms.sourcegitcommit: b0b1b8e1182cce93929d409706cdaa99ff24fdee
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 07/23/2019
ms.locfileid: "68387593"
---
# <a name="eye-gaze-on-hololens-2"></a>目视-注视 HoloLens 2
HoloLens 2 允许开发人员使用有关用户正在查看的信息的功能, 使开发人员能够在全息体验内实现新的上下文和人工理解。 本页告诉开发人员如何从各种用例的目视跟踪中获益, 以及在设计基于目视的用户界面时要查找的内容。 


## <a name="device-support"></a>设备支持

<table>
<colgroup>
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
    <col width="25%" />
</colgroup>
<tr>
     <td><strong>功能</strong></td>
     <td><a href="hololens-hardware-details.md"><strong>HoloLens（第 1 代）</strong></a></td>
     <td><strong>HoloLens 2</strong></td>
     <td><a href="immersive-headset-hardware-details.md"><strong>沉浸式头戴显示设备</strong></a></td>
</tr>
<tr>
     <td>眼睛-注视</td>
     <td>❌</td>
     <td>✔️</td>
     <td>❌</td>
</tr>
</table>

## <a name="use-cases"></a>用例
眼动跟踪可让应用程序实时跟踪用户正在注视的位置。 以下用例介绍了混合现实中的目视跟踪可能会发生的一些交互。
请记住,[混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)对于使用目视跟踪提供几个有趣和强大的示例非常有用, 例如, 快速和轻松地支持目视的目标选项, 以及基于用户查看的内容。 

### <a name="user-intent"></a>用户意图    
有关用户所在位置和内容的信息**为其他输入**提供了强大的上下文, 例如语音、动手和控制器。
可在各种任务中使用此信息。
例如,**在整个场景**中, 这种情况的范围包括: 只需查看一个全息影像[并显示 "](gaze-and-commit.md)选择" (还可以看到 "选择"), 或者说 "put ...", 然后查找用户要放置到的位置。全息图, 如 ".。。出现 "。 在[混合现实工具包 - 视线支持的目标选择](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html)和[混合现实工具包 - 视线支持的目标定位](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html)中可以找到相关示例。

此外, 用户意向的示例可能包括使用用户查看的信息来增强使用所介绍的虚拟代理和交互式全息影像。 例如, 虚拟代理可能会根据当前查看的内容来调整可用选项及其行为。 

### <a name="implicit-actions"></a>隐式操作
隐式操作的类别与用户意图密切相关。
其思路是, 全息影像或用户界面元素以某种 instinctual 的方式做出反应, 甚至可能不会像用户同时与系统交互, 而是让系统和用户保持同步。例如,**基于眼睛的自动滚动**, 用户可在其中读取文本, 因为文本会继续与用户的眼睛一起滚动或流动。 这种情况的一个关键方面是, 滚动速度可适应用户的读取速度。
另一个示例是**支持目视的缩放和平移**, 其中用户可以感觉到他或她的聚焦复杂度完全接近。 触发缩放和控制缩放速度可以通过语音或手写输入来控制, 这对于向用户提供控制感受, 同时避免被淹没非常重要。 下面将更详细地讨论这些设计指南。 放大后, 用户可以顺利地执行操作, 例如, 街道的学习过程只需使用其眼睛来浏览其邻居即可。
[混合现实工具包 - 视线支持的导航](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html)示例中可以找到此类交互的演示。

_隐式操作_的其他用例包括:
- **智能通知：** 当你专心浏览某段内容时，突然弹出的通知是否让你觉得很恼火？ 考虑到用户正在关注的内容, 你可以通过从当前 gazing 用户的位置偏移通知来更好地进行此体验。 这会限制干扰, 并在用户完成读取后自动将其关闭。 
- **体贴入微的全息影像：** 在 gazed 时, 会对影像进行细微反应。 这种情况的范围包括: 从稍微光亮的 UI 元素到缓慢的百花齐放花给虚拟宠物, 开始回顾用户, 或尝试避免在长时间的琢磨后出现用户的眼睛。 这种交互可能会在应用程序中提供更有趣的连接和满意度。

### <a name="attention-tracking"></a>注意力跟踪   
有关用户所在位置或内容的信息是一个非常强大的工具, 可用于评估设计的可用性, 并确定有效工作流中的问题。 目视跟踪可视化和分析是各种应用程序领域的常见做法。 在 HoloLens 2 中, 我们提供了一种新的维度来理解, 因为3D 全息图可以放置在真实的上下文中并进行相应的评估。 [混合现实工具包](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html)提供了用于记录和加载目视跟踪数据的基本示例, 以及如何对其进行可视化。

此区域中的其他应用程序可以包括: 
-   **远程目视视觉视觉对象:** 可视化远程协作者正在寻找的内容, 以确保正确理解并遵循说明。
-   **用户调研：** 注意跟踪可用于探索初级用户与专家用户直观地分析内容的方式, 或者对复杂任务 (例如分析医疗数据或操作机械) 进行实时协调。
-   **训练模拟和性能监视：** 更有效地识别执行流中的瓶颈，实践并优化任务的执行。
-   **设计评估、广告和营销调查：** 在 evaluateing 网站和产品设计时, 目视跟踪是市场调查的常用工具。

### <a name="additional-use-cases"></a>其他用例
- **游戏：** 是否曾经想过拥有超能力？ 机会来了！ 可以通过起始在 levitate 全息影像。 从眼睛发射激光束。 将敌人变成石子, 或将其冻结。 使用 X 光透视来扫描建筑物。 没有做不到，只有想不到！  

- **富有表现力的虚拟形象：** 目视跟踪通过使用实时跟踪日期对虚拟形象的眼睛进行动画处理, 使其看起来用户正在寻找的内容, 以更具表现力的方式跟踪三维头像。 它还能添加更多的眨单眼和眨眼表情。 

- **文本输入：** 目视跟踪可用作低工作量文本输入的替代方案, 特别是当语音或手不方便使用时。 


## <a name="eye-tracking-api"></a>眼动跟踪 API
在深入了解有关目视目视交互的特定设计准则的详细信息之前, 我们想要简要指出 HoloLens 2 目视跟踪器 API 为开发人员提供的功能。 它提供一只眼睛的眼睛和方向, 提供大约_30 FPS_的数据。 

预期的眼睛在 ca 中。 1.0-1.5 度 (以视觉角度表示的实际目标)。 预期存在轻微的不精确性，因此，应该围绕此下限值规划好一定的余隙。 下面会更详细地讨论此问题。 要准确运行眼动跟踪，每个用户需要完成眼动跟踪用户校准。 

![2 米远处的最佳目标大小](images/gazetargeting-size-1000px.jpg)<br>
*以2米距离为目标的最佳目标大小*
<br>
<br>
可以通过 "EyesPose" 访问[目视跟踪 API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) 。 

## <a name="eye-gaze-design-guidelines"></a>目视设计准则
构建利用快速移动的视线定位功能的交互式应用可能有难度。 在本部分中, 我们总结了在设计应用程序时要考虑的主要优点和挑战。 

### <a name="benefits-of-eye-gaze-input"></a>目视输入的优点
- **高速指向。** 眼部肌肉在人体中是反应速度最快的肌肉。 

- **不费力。** 几乎没有任何身体动作。 

- **隐含性。** 用户通常会将其描述为 "构思阅读", 有关用户的目视变动的信息使系统可以知道用户打算参与哪个目标。 

- **备选的输入通道。** 眼睛可为用户提供强大的支持输入, 并根据用户的手眼协调从用户的经验中生成。

- **视觉注意力。** 另一个重要的优点是可以推断出用户正在关注的内容。 这可以帮助不同的应用程序领域, 从更有效地评估不同设计, 到协助的用户界面, 增加了社交提示以进行远程通信。

简而言之, 使用眼睛眼睛作为输入可提供快速且简单的上下文信号。 与其他输入 (如*语音*和*手动*输入) 相结合时, 此功能特别强大, 可以确认用户的意图。


### <a name="challenges-of-eye-gaze-as-an-input"></a>眼睛为输入的挑战
如果有大量的强大功能, 就有了许多责任。
虽然眼睛可用于创建符合用户体验, thata 使你感觉像是 superhero, 但是, 了解这一点也很重要。 下面讨论了一些需要考虑的挑战, 以及如何解决这些*难题*: 

- **眼睛为 "始终打开"** 打开眼睛护盖后, 眼睛开始 fixating 环境中的东西。 对每个外观进行操作并意外发出操作, 因为您查看的内容太长会导致 unsatisfying 体验。
这就是我们建议你将眼睛与*声音命令*、*笔势*、*按钮单击*或扩展停留结合起来以触发选择目标的原因。
此解决方案还允许使用一种模式, 在该模式下, 用户可以自由地查找, 而不会因触发某些事情而 involuntarily。 设计视觉和听觉反馈（只需注视某个目标）时，也应考虑到此问题。
不要使用即时弹出式效果或惊悚的声音来让用户感到不知所措。 个很微妙为 key。 稍后在谈到设计建议时，我们会进一步讨论一些最佳做法。

- **观察与控制**假设您想要在墙壁上精确地伸直照片。 你会参照它的边框和四周，检查它是否平齐。 现在, 如果您想要使用眼睛作为输入来移动图片, 则可以想象出如何实现此目的。 有点难度，对不对？ 这介绍了输入和控制需要时, 眼睛的双重角色。 

- **点击之前离开目标：** 对于快速目标选择, 研究表明, 用户的眼睛可以在结束手动单击之前继续进行 (例如, airtap)。 因此, 必须特别注意的是, 将快速目视眼睛的信号与慢速控制输入 (例如语音、双手、控制器) 进行同步。

- **小目标：** 当您尝试读取只是太小而无法阅读的文本时, 您是否知道这种感觉？ 这会使您感到非常紧张, 因为您尝试重新调整您的眼睛, 使您能够更好地专注。
当你强制用户使用目视目标选择在你的应用程序中太小的目标时, 你可以在用户中调用这种感觉。
在设计方面，为了给用户建立一种愉悦舒适的体验，我们建议目标至少在 2° 的视角范围内，最好是再大一些。

- **眼睛不规则的运动**我们的眼睛执行从固定到固定的快速移动。 观察录制的眼部运动扫描路径时你会发现，这些路径看起来是不规则的。 与头部跟踪视线和手部运动相比，眼睛的移动速度飞快，是本能跳动的。  

- **跟踪可靠性：** 当眼睛在光线变化的环境中适应新的条件时，眼动跟踪准确度可能会略有下降。
虽然这应该不会影响应用程序的设计, 因为准确性应该在2°限制范围内, 因此, 用户可能需要运行其他校准。 


## <a name="design-recommendations"></a>设计建议
下面是基于目视输入的所述优点和挑战的特定设计建议列表:

1. **眼睛! = 机头:**
    - **考虑快速且不规则的眼部运动是否适合你的输入任务：** 虽然我们的快速和不规则的眼睛非常适合于在我们的视图 (FoV) 领域快速选择目标, 但它不适用于需要平滑输入轨迹的任务 (例如, 绘制或 encircling 批注)。 在这种情况下，应该首选手部或头部指向。
  
    - **避免直接将某些内容附加到用户的眼睛, 如滑块或光标。**
如果是游标, 这可能会导致 "fleeing cursor" 效果, 原因是预计的眼睛眼睛略有偏差。 如果是滑块, 则它可能与用眼睛控制滑块的双角色冲突, 同时也需要检查对象是否位于正确的位置。 简而言之, 用户可能会被淹没和分散注意力, 尤其是在信号对于该用户不精确的情况下。 
  
2. **将眼睛与其他输入组合在一起:** 将眼睛跟踪与其他输入 (如手势、语音命令或按钮按下) 的集成具有以下优点:
    - **可以自由观察：** 考虑到我们的主要角色是观察我们的环境, 这是一项重要的用户, 而不触发任何 (视觉、听觉等) 反馈或操作。 
    将目视跟踪与其他输入控件结合起来允许在目视跟踪观察和输入控制模式之间平稳过渡。
  
    - **强大的上下文提供程序：** 使用有关用户在 uttering 语音命令或执行手手势时所处的位置和内容的信息, 可以在视图字段间无缝排列输入。 例如：发出“放在这里”语音命令后，只需注视某个目标和目的地，就能快速顺畅地在场景中选择和放置全息影像。 

    - **同步多模输入的需求（“点击之前离开”问题）：** 将快速目视运动与更复杂的附加输入 (例如长的语音命令或手势) 相结合, 就能在完成附加输入命令之前继续眼睛的风险。 因此，如果你要创建自己的输入控件（例如自定义手势），请务必记录此输入的发生时间或近似持续时间，使之与用户过去注视的物件相关联。
    
3. **眼动追踪输入的微妙反馈：** 当查看目标以指示系统正在按预期工作时提供反馈, 但应保持微妙, 这非常有用。 这可能包括缓慢混合、放大和缩小、视觉对象突出显示或执行其他细微目标行为 (如缓慢增加目标), 以指示系统正确检测到用户正在查看目标而不不必要地中断用户的当前工作流。 

4. **避免强制使用非自然的眼部运动作为输入：** 不要强制用户执行特定的目视运动 (注视手势) 来触发应用程序中的操作。

5. **考虑不精确性：** 我们区分对用户显而易见的两种类型的 imprecisions: 偏移和抖动。 解决偏移量的最简单方法是提供足够大的目标来与进行交互。 建议使用大于2°的视觉角度作为参考。 例如, 当您拉伸 arm 时, 缩略图约为2°。 因此，我们的指导如下：
    - 不要强制用户选择 "小目标"。 研究表明, 如果目标足够大, 并且系统设计良好, 则用户会将其交互描述为轻松和神奇。 如果目标太小，则用户就会将体验描述为费力、令人沮丧。
   

## <a name="see-also"></a>请参阅
* [头部凝视并提交](gaze-and-commit.md)
* [在 DirectX 中的打印头和眼睛](gaze-in-directx.md)
* [目视看 Unity (混合现实工具包)](https://aka.ms/mrtk-eyes)
* [手势](gestures.md)
* [语音输入](voice-design.md)
* [运动控制器](motion-controllers.md)
* [舒适](comfort.md)
